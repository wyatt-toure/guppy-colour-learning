---
title: "Analysis for 'Colour biases in learned foraging preferences in Trinidadian guppies'"
subtitle: "Experiment 2 Data Preparation and Analysis"
author: 
  - name: "M. Wyatt Toure^1^, Simon M. Reader^1^"
    affiliation: "^1^McGill University, Department of Biology, 1205 Docteur Penfield, Montreal, Quebec H3A 1B1, Canada" 
date: "Last Update: `r format(Sys.Date(), '%b %d %Y')`"
output:
  bookdown::html_document2:
    includes:
      in_header: docs/header.html    
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false      
    number_sections: false
    split_by: section
    css: styles.css
    
bibliography: ["references/references.bib"]
csl: references/elife-citation-style.csl    
link-citations: yes

knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = 'docs/analysis-experiment-2')
  })
---

***

## Overview

This page reports the analyses for the second experiment described in 'Colour
biases in learned foraging preferences in Trinidadian guppies'. The code run to
produce the results is included on the page along with explanations of what the
code is doing and why. The raw R script to reproduce the data preparation,
analysis, figures, and this page are in
[analysis-experiment-2.Rmd](https://github.com/wyatt-toure/guppy-colour-learning-project/blob/main/analysis-experiment-2.Rmd).
Note the code blocks that produce the figures and tables are not shown on this
page as they are rather long, however the code to produce the figures and tables
can also be seen in
[analysis-experiment-2.Rmd](https://github.com/wyatt-toure/guppy-colour-learning-project/blob/main/analysis-experiment-2.Rmd).
To get straight to the results go to the [Models](#models) section. To see how
to reproduce these results please visit the [How to Reproduce the
Results](https://github.com/wyatt-toure/guppy-colour-learning#how-to-reproduce-the-results)
section of the README.

```{r library-prep, include=FALSE}
# Loading required packages
library(lme4)
library(knitr)
library(rmarkdown)
library(tidyr)
library(lmerTest)
library(ggplot2)
library(ggpubr)
library(DHARMa)
library(dplyr)
library(broom)
library(broom.mixed)
library(knitr)
library(emmeans)
library(report)
library(glmmTMB)
library(MASS)
library(googledrive)
library(stringr)
source("R/format-p-value.R")
source("R/rename-lme4-model.R")
source("R/geom-flat-violin.R")
source("R/read-and-format-ethovision-data.R")
source("R/report-est-plus-minus-std-error.R")
source("R/report-ci.R")
```

***

## Data preparation 

In this section we detail the steps taken to process the raw data produced by
processing video footage with automated tracking from Noldus EthoVision
[@noldus2001EthoVisionVersatileVideo]. The raw data can be found in the
[`data/experiment-2-raw-data/`](https://github.com/wyatt-toure/guppy-colour-learning-project/tree/main/data/experiment-2-raw-data)
directory. They are composed of `.xlsx` files exported from EthoVision XT
Version 11. Each trial is in a separate `.xlsx` file. The full processed data
are available as a `.csv` file in the file
[`colour-learning-experiment-2-full-data.csv`](https://github.com/wyatt-toure/guppy-colour-learning-project/blob/main/data/colour-learning-experiment-2-full-data.csv).
Descriptions of the variables found in the data set are given in the
Experiment 2 Metadata section of
[`metadata.md`](https://github.com/wyatt-toure/guppy-colour-learning/blob/main/data/metadata.md#experiment-2-metadata)
file.

### Downloading data

To prepare the data first we download the raw data files from the Google Drive
folder they are stored in. We make use of the tidyverse package `googledrive` to
do this. We put `googledrive` into a de-authorized state so we can access public
Google Drive resources without a Google sign-in. We then get the list of files
that are present in the Google drive directory and use a `for()` loop which
downloads each file using the `drive_download()` function. The data are
downloaded to the
[`data/experiment-2-raw-data/`](https://github.com/wyatt-toure/guppy-colour-learning-project/tree/main/data/experiment-2-raw-data)
directory.

```{r data-download, message=FALSE, warning=FALSE, eval=FALSE}
# Downloading data from Google drive

## Put googledrive into a de-authorized state
drive_deauth()

## Store link to data folder
data_folder_link <- "https://drive.google.com/drive/folders/1A8NRlBMQ-BfkgNHzEpmw6hEbgePJncLj?usp=sharing"

## Get id for data folder
data_folder_id <- drive_get(as_id(data_folder_link))

## Store the list of file names and ids found in the data folder
data_files <- drive_ls(data_folder_id)

## Loop through and download each file
for (file_x in 1:length(data_files$name)) {
      drive_download(
        as_id(data_files$id[file_x]),
        path = str_c("data/experiment-2-raw-data/",data_files$name[file_x]),
        overwrite = TRUE)
}
```

### Formatting data

Next we read in and format the raw `.xlsx` files from EthoVision which are in
`data/experiment-2-raw-data/` using one of my custom functions,
`read_and_format_ethovision_data()`. The code for this can be seen in
[`read-and-format-ethovision-data.R`](https://github.com/wyatt-toure/guppy-colour-learning-project/blob/main/R/read-and-format-ethovision-data.R).


```{r data-read, message=FALSE}
# Reading in Data
full_data <- read_and_format_ethovision_data("data/experiment-2-raw-data/")
```

Next we add the rewarding object colour treatments to the correct guppy IDs that
were established *a priori* in
[`treatment-object-side-assignment.Rmd`](https://github.com/wyatt-toure/guppy-colour-learning-project/blob/main/treatment-object-side-assignment.Rmd).
The treatments are represented by the variable `rewarding.object.colour`.

```{r treatment-assignment}
## Assigning treatments
full_data <- full_data %>%
  mutate(
    rewarding.object.colour =
      case_when(
        id == "1a" ~ "blue", id == "1b" ~ "green",
        id == "2a" ~ "blue", id == "2b" ~ "blue",
        id == "3a" ~ "blue", id == "3b" ~ "green",
        id == "4a" ~ "green", id == "4b" ~ "green",
        id == "5a" ~ "green", id == "5b" ~ "blue",
        id == "6a" ~ "green", id == "6b" ~ "green",
        id == "7a" ~ "blue", id == "7b" ~ "blue",
        id == "8a" ~ "green", id == "8b" ~ "blue"
      )
  )
```

All the variables for the data set are read in as characters due to the
`read_excel()` call in `read_and_format_ethovision_data()`, so we need to
convert them to their appropriate data structures for the analysis. Variables
are converted to either factors or numerics where appropriate using the
`lapply()` function which applies a function over a vector. We apply the
`as.factor()` function to categorical variables identified in the `Factors`
vector and the `as.numeric()` function to the numerical variables identified in
the `Numerics` vector.

For the latency measures, dashes in the raw data sheet indicate that an
individual never visited the zone of interest. In being converted to numerics
these values are changed to NAs. We convert these values to the maximum value
which is the trial duration (300 seconds) using the `tidyr` function
`replace_na()`.

```{r variable-conversion, warning=FALSE}
# Converting variables

## Factors
Factors <- c("ate", "id", "object.side", "rewarding.object.colour", "object.pair")
full_data[Factors] <- lapply(full_data[Factors], as.factor)

## Numeric
Numerics <- c(
  "trial", "left.object.visits", "time.with.left.object",
  "left.object.latency", "right.object.visits", "time.with.right.object",
  "right.object.latency", "periphery.visits", "time.in.periphery",
  "latency.to.periphery", "center.visits", "time.in.center",
  "latency.to.center", "distance.moved", "mean.velocity"
)
full_data[Numerics] <- lapply(full_data[Numerics], as.numeric)

## Latency NA replacement
full_data <- full_data %>%
  replace_na(
    list(
      left.object.latency = 300,
      right.object.latency = 300,
      latency.to.periphery = 300,
      latency.to.center = 300
    )
  )
```

### Variable creation

New variables and measures need to be created from the variables present in the
raw data sheets. We do this using the `mutate()` and `case_when()` functions
from the tidyverse package `dplyr`. First we invert the object side because the
camera image is reversed from the perspective of the experimenter. We then
create the variables `time.with.trained.object` and `time.with.untrained.object`
by identifying whether the left or right object is the reward object.

The preference metrics `green.object.preference` and
`rewarding.object.preference` are created by subtracting the time spent near the
blue object from the time spent near the green object and subtracting the time
spent near the untrained object from the time spent near the trained object
respectively.

`time.with.both.objects` is obtained by summing the time spent near the left and
the right object. `total.time` is obtained by summing the `time.in.periphery`
with the `time.in.center`. `total.time` should be close to 300 since trials
last 5 minutes (300 seconds).

We also create the variable `trial.type` to identify whether a trial is a test
trial (unreinforced), training trial (reinforced), or refresher trial
(reinforced).

```{r variable-creation, warning=FALSE, message=FALSE}
# Creating new variables

## Inverting object side
full_data <- full_data %>%
  mutate(
    reward.object.side =
      as.factor(
        case_when(
          object.side == "left" ~ "right",
          object.side == "right" ~ "left"
        )
      )
  )

## Time with trained object
full_data <- full_data %>%
  mutate(
    time.with.trained.object =
      case_when(
        reward.object.side == "left" ~ time.with.left.object,
        reward.object.side == "right" ~ time.with.right.object
      )
  )

## Time with untrained object
full_data <- full_data %>%
  mutate(
    time.with.untrained.object =
      case_when(
        reward.object.side == "left" ~ time.with.right.object,
        reward.object.side == "right" ~ time.with.left.object
      )
  )

## Green object preference
full_data <- full_data %>%
  mutate(
    green.object.preference =
      case_when(
        rewarding.object.colour == "green" ~
        time.with.trained.object - time.with.untrained.object,
        
        rewarding.object.colour == "blue" ~
        time.with.untrained.object - time.with.trained.object
      )
  )

## Rewarding object preference
full_data <- full_data %>%
  mutate(
    rewarding.object.preference =
      time.with.trained.object - time.with.untrained.object
  )

## Proportionanl Rewarding object preference
full_data <- full_data %>%
  mutate(
    prop.rewarding.object.preference =
      time.with.trained.object / (time.with.trained.object + time.with.untrained.object)
  )

## Time with both objects
full_data <- full_data %>%
  mutate(
    time.with.both.objects =
      time.with.left.object + time.with.right.object
  )

## Total time
full_data <- full_data %>%
  mutate(
    total.time =
      time.in.center + time.in.periphery
  )

## Trial type
full_data <- full_data %>%
  mutate(
    trial.type =
      as.factor(
        case_when(
          trial == 0 | trial == 21 | trial == 23 | 
            trial == 25 | trial == 27 | trial == 29 ~ "test",
          trial > 0 & trial < 21 ~ "training",
          trial == 22 | trial == 24 |  trial == 26 | trial == 28 ~ "refresher"
        )
      )
  )
```

We now create a variable which establishes how many trials an individual guppy
fed in during the 20 training trials. We do this by creating a variable called
`fed` which can be either True or False. Every time `ate`, the variable which
indicates whether an individual ate during that trial or not, is equal to yes
fed is set to True. When this is not the case fed is set to False. We then
retrieve only the rows of fed which have fed set to True. We remove the column
fed to keep only the counts. We then add the feeding values to the full data
set. We end by extracting a smaller data set that contains only the variables
`id`, `feeding.count`, and `rewarding.object.colour` treatment which will be
used for model 4.

```{r feeding-data-prep}
# Group by ID and count the number of sessions in which an individual ate during training
feeding <- full_data %>%
  filter(trial.type == "training") %>% 
  group_by(id) %>%
  count(fed = ate == "yes")

# Count only the yeses
feeding <- feeding %>%
  filter(fed == "TRUE")

# Remove the column feeding.count to keep only the counts
feeding <- feeding %>%
  dplyr::select(-fed)

# Add the feeding values to the main data frame so I can get treatment IDs
full_data <- left_join(full_data, feeding, by = "id") %>%
  replace_na(list(n = 0)) %>% 
  rename(training.feeding.count = n)

# Extract id, feeding count, and rewarding object colour treatment
feeding_data <- full_data %>%
  filter(trial == 0) %>% 
  dplyr::select(id, training.feeding.count, rewarding.object.colour)
```

After trial 21 all guppies were weighted. In the next chunk of code we
programmatically assign the weights (measured in grams) to their respective
guppy IDs.

```{r}
## Assigning weights
full_data <- full_data %>% 
  mutate(
    weight =
      case_when(
        id == "1a" ~ 0.29, id == "1b" ~ 0.10,
        id == "2a" ~ 0.20, id == "2b" ~ 0.11,
        id == "3a" ~ 0.20, id == "3b" ~ 0.12,
        id == "4a" ~ 0.21, id == "4b" ~ 0.11,
        id == "5a" ~ 0.18, id == "5b" ~ 0.13,
        id == "6a" ~ 0.15, id == "6b" ~ 0.11,
        id == "7a" ~ 0.31, id == "7b" ~ 0.13,
        id == "8a" ~ 0.17, id == "8b" ~ 0.14
      )
  )
```

### Exporting processed data and creating data subsets

Finally we export the full data set as a `.csv` file to future proof the full
data sheet in a plain text, machine-readable format. `row.names` is set to
`FALSE` so that the index column is not exported into the `.csv` file.

```{r data-export}
write.csv(full_data, 
          file = "data/colour-learning-experiment-2-full-data.csv",
          row.names = FALSE)
```

For our analyses we made the decision to omit an individual that did not feed
throughout all reinforced trials. This fish appeared otherwise normal and
healthy but did not eat in any of the 20 training trials and also did not eat in
any of the 4 refresher trials which is quite aberrant behaviour. We therefore
believe that data from this individual is not likely to be informative as the
fish has not participated in the experiment in a meaningful way.

```{r}
full_data <- full_data %>% filter(training.feeding.count > 0)
```

To conduct the analyses we planned, we create subsets of the full data set that
are restricted to the training trials (reinforced), the test trials
(unreinforced), and the initial test trial (unreinforced) using the `filter()`
function from `dplyr`. We change trial to a factor for the unreinforced test
trial data subset since we are interested in comparing the levels of trial to
each other rather than looking at differences in trends like we are with the
larger training data subset. Trial in the training data subset is coded as
integer to allow us to look at trends in the shift of rewarding object
preference during training.

```{r data-subset, warning=FALSE, message=FALSE}
# Restrict data to only the baseline data
baseline_data <- full_data %>%
  filter(trial == 0)

# Restrict data to training data
training_data <- full_data %>%
  filter(trial.type == "training")

# Restrict data to only the baseline and re-test data
test_data <- full_data %>%
  filter(trial.type == "test")

# Change trial to factor for test trials
test_data$trial <- as.factor(test_data$trial)

# Change trial to integer for training trials
training_data$trial <- as.integer(training_data$trial)
```

### Figures directory

This script will generate figures but to store them we need to create specific
directories that have been hard coded into the script. We create the `figs/`
directory and all the subdirectories within it using the next line of code. Now
all figures that are created in this script are accessible as individual files
in the `figs/` directory. If this script is run multiple times it will return a
warning saying that the directory already exists. However, this is not
problematic since the figures are always regenerated by running the script so we
set `showWarnings` to `FALSE`.

```{r}
dir.create(file.path("figs/exp-2/exp-2-residual-plots"),
           recursive = TRUE,
           showWarnings = FALSE)
```

***

## Models

We analysed the data from our experiment using linear, linear mixed effect,
generalized linear mixed effect, and generalized linear models with the `lm`,
`lmer()`, `glmmTMB()`, and `glm.nb` functions from the `stats`, `lme4`,
`glmmTMB`, and `MASS` packages respectively. P-values and effective degrees of
freedom were obtained using the `lmerTest` package which uses Satterthwaite's
degrees of freedom method [@kuznetsova2017LmerTestPackageTests]. Model
residuals were checked they met distributional assumptions with the `DHARMa`
package. The 'See Model Residuals' button below the model formulas can be
clicked to see the residual diagnostic plots produced by `DHARMa` for that
particular model.

### Model 1 -  Preference for the green object at baseline

This first model contains the data for all individual guppies during the initial
test. We looked at the green object preference of all guppies in an intercept
only model to see if the green object preference at baseline was significantly
different from zero. `green.object.preference` is the time spent near the green
object subtracted by the time spent near the blue object.

```{r model-1, echo=TRUE}
baseline_data_model <-
  lm(green.object.preference ~ 1,
    data = baseline_data
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName"> See Model 1 Residuals </button>  
<div id="BlockName" class="collapse"> 

```{r, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = baseline_data_model)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "exp-2-model-1-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/exp-2/exp-2-residual-plots",
  device = "png",
  dpi = 300
)
```

</div>

\

##### Result

```{r tidying-model-1, echo=FALSE, message=FALSE}
# Setting table row names
baseline_table_row_name_vec <- c("Intercept")

# Converting data frame to tibble
tidy_baseline_model <- broom.mixed::tidy(baseline_data_model)

# Changing tibble header names
tidy_baseline_model <- rename_tidy_lme4_cols(tidy_baseline_model)

# Changing tibble row names
tidy_baseline_model[1:1, 1] <- baseline_table_row_name_vec
```

```{r,  results=TRUE, echo=FALSE}
knitr::kable(tidy_baseline_model %>%
  mutate_if(is.numeric, round, digits = 3))
```

Before training began, there was no significant difference in the time spent
near the green versus the blue object across all guppies (green object
preference: `r report_est_and_std_error(tidy_baseline_model, rounding = 2)`
seconds, p = `r tidy_baseline_model$'P value' %>% round(3)`).

```{r baseline-pref-plot, echo=FALSE, message=FALSE, echo=FALSE, fig.cap="Preference for the green object relative to the blue object across all guppies at baseline. Negative values represent more time spent with the blue object, positive values indicate more time spent with the green object. Data are means ± 95% CI", fig.id="baseline-pref-plot",  warning=FALSE, message=FALSE}
###### Baseline green object preference plot ######
baseline_data_x_axis_label <- "Initial Test"
ggplot(
  baseline_data,
  aes(
    x = as.factor(trial),
    y = green.object.preference
  )
) +
  theme_minimal() +
  ylab("Green object preference (sec)") +
  xlab("") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  ) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  geom_jitter(width = 0.04, alpha = 0.3) +
  stat_summary(
    geom = "point",
    fun = "mean",
    size = 4.5,
    shape = 15
  ) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci", position = position_dodge(width = 0), width = 0.1
  ) +
  scale_x_discrete(labels = baseline_data_x_axis_label)

ggsave(
  filename = "exp-2-model-1-baseline-data-plot.png",
  path = "figs/exp-2",
  device = "png",
  dpi = 300
)
```

***

### Model 2 -  Preference for the rewarding object during training

To see how fish behaved during training our second model asks whether the
preference for the rewarding object changes throughout training and whether the
change in rewarding object preference is different between the treatments.

```{r model-2, echo=TRUE}
training_data_model <-
  lmer(rewarding.object.preference ~ trial * rewarding.object.colour + (1 | id),
    data = training_data
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName2"> See Model 2 Residuals </button>  
<div id="BlockName2" class="collapse">  

```{r, message=FALSE}
# Residual diagnostics
simulationOutput <- simulateResiduals(
  fittedModel = training_data_model,
  n = 1000
)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "exp-2-model-2-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/exp-2/exp-2-residual-plots",
  device = "png",
  dpi = 300
)
```

There is a slight deviation in the lower quantile but no indication in the
residual plot of a gross model misfit.

</div>

```{r tidying-model-2, echo=FALSE, message=FALSE}
# Setting table row names
training_model_table_row_name_vec <- c(
  "Intercept",
  "Reward object colour",
  "Trial",
  "Rewarding object colour X Trial"
)

# Converting data frame to tibble
tidy_training_data_model <- broom.mixed::tidy(training_data_model)

# Formatting p value
tidy_training_data_model$p.value <- format_p_value(tidy_training_data_model$p.value)

# Changing tibble header names
tidy_training_data_model <- rename_tidy_lme4_cols(tidy_training_data_model)

# Changing tibble row names
tidy_training_data_model[1:4, 3] <- training_model_table_row_name_vec
```

\

##### Results

```{r,  results=TRUE, echo=FALSE}
knitr::kable(tidy_training_data_model[1:4, ] %>%
  dplyr::select(-group, -effect) %>%
  mutate_if(is.numeric, round, digits = 3))
```

There was a significant interaction effect between trial and rewarding object
colour (`r report_est_and_std_error(tidy_training_data_model, rounding = 2)[4]`
seconds, p = `r tidy_training_data_model$'P value'[4]`) indicating that the
change in rewarding object preference has a different trend depending on the
rewarding object colour. We used the `emtrends()` function from `emmeans` to
estimate and compare the trends.

```{r}
training_data_model_trends <-
  emtrends(training_data_model,
    pairwise ~ rewarding.object.colour,
    var = "trial"
  )
```

```{r, echo=FALSE}
training_data_model_trends_values <- training_data_model_trends$emtrends %>%
  as.data.frame()

kable((training_data_model_trends_values %>%
  rename(
    "Rewarding object colour" = rewarding.object.colour,
    "Trial trend" = trial.trend,
    "Std. Error" = SE,
    "Lower CL" = lower.CL,
    "Upper CL" = upper.CL
  )) %>%
  mutate_if(is.numeric, round, digits = 3))
```


Guppies that were trained to green objects increased their relative preference
for rewarding objects by
`r training_data_model_trends_values$trial.trend[2] %>% round(1)` seconds on
average each trial whereas guppies trained to blue objects increased their
relative preference for rewarding objects by
`r training_data_model_trends_values$trial.trend[1] %>% round(1)` seconds on
average each trial. Thus, while both groups increased their preference for their
respective rewarding objects over training, green trained guppies increased
their preference at a rate that was
`r (training_data_model_trends_values$trial.trend[2]/training_data_model_trends_values$trial.trend[1]) %>% round(1)`x
faster than blue trained guppies (Figure \@ref(fig:colour-pref-training-plot)).

```{r, colour-pref-training-plot, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Relative preference for the green object in both treatments during training trials (trials 1-20). Negative values represent more time spent with the blue object, positive values indicate more time spent with the green object. Light lines connect individuals across trials. Subjects were consistently rewarded for approaching the blue object (dashed lines) or the green object (solid lines).", fig.id="colour-pref-training-plot"}
ggplot(
  training_data,
  aes(
    x = trial,
    y = green.object.preference,
    color = rewarding.object.colour,
    shape = rewarding.object.colour,
    linetype = rewarding.object.colour
  )
) +
  theme_minimal() +
  ylab("Green object preference (sec)") +
  xlab("Trial") +
  labs(col = "Rewarding object colour") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  ) +
  scale_color_manual(values = c("#2980b9", "#27ae60")) +
  scale_linetype_manual(values = c("longdash", "solid")) +
  scale_shape_manual(values = c(15, 16)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(alpha = 0.3) +
  geom_line(aes(group = id), alpha = 0.2) +
  scale_x_continuous(breaks = c(1:20)) +
  scale_y_continuous(breaks = seq(-300, 300, by = 100)) +
  geom_smooth(method = "lm", se = TRUE, alpha = 0.25) +
  stat_summary(fun = "mean", size = 0.8)

 ggsave(
  filename = "exp-2-model-2-colour-pref-training-plot.png",
  path = "figs/exp-2/",
  device = "png",
  dpi = 300
)
```

***

### Model 3 - Preference for the rewarded object during testing {#model-3}

To determine whether learning had occurred we used the initial preference for
the rewarding object colour as a control and compared each probe test trial to
this control trial for each treatment. To do so we fit a generalized linear
mixed effects model with a Gaussian distribution with fixed effects of trial and
rewarding object colour (green versus blue), a random effect of individual
identity, and a response variable of rewarding object preference using the
package [`glmmTMB`](https://cran.r-project.org/web/packages/glmmTMB/index.html).
To control for heterogeneous variance across trials we additionally modelled the
variance due to trial.

```{r model-3, echo=TRUE}
test_data_model_glm <-  
  glmmTMB(rewarding.object.preference ~  
            trial * rewarding.object.colour + (1 |id) +
            diag(0 + trial |id),
  data = test_data,
  family = gaussian
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName3"> See Model 3 Residuals </button>  
<div id="BlockName3" class="collapse">  

```{r, include=TRUE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = test_data_model_glm, n = 1000)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "exp-2-model-3-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/exp-2/exp-2-residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>

\

##### Results

Given our factor `trial` has more than two levels, six in this case, we produced
an ANOVA table for our model. We used the `Anova()` function from the `car`
package to produce a type-III ANOVA table based on Wald chi-square tests for
[Model 3](#model-3).

```{r, eval=FALSE}
car::Anova(test_data_model_glm, type = "III")
```

```{r model-3-anova-table, message=FALSE, echo=FALSE}
model_3_anova_table <- car::Anova(test_data_model_glm, type = "III") 


rownames(model_3_anova_table) <- c("Intercept", "Trial", 
                                   "Rewarding object colour", 
                                   "Trial X Rewarding object colour")
model_3_anova_table %>% 
  tibble::rownames_to_column(var = "Factor") %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  rename("P Value" = 4) %>% 
  kable(caption = "Type 3 ANOVA table for the effects of rewarding object colour, trial, and their interaction on rewarding object preference across the six test trials in experiment 2")
```

There is an overall interaction effect between trial and rewarding object colour
for the test trials (p = `r model_3_anova_table$'Pr(>Chisq)'[4] %>% round(3)`).
To investigate the interaction effect and determine where the differences in
performance were apparent we conducted treatment versus control post-hoc tests.
Here we compared the estimated marginal means of rewarding object preference for
each post-training test trial to the estimated marginal mean of the initial
rewarding object preference for each rewarding object colour respectively to
establish whether learning had occurred. Learning was assumed to have occurred
if the change in preference between the initial trial (serving as the control)
and the test trial was significant. We used a multivariate t adjustment to
correct p values in light of multiple comparisons 
[@hothorn2008SimultaneousInferenceGeneral].

```{r post-hoc-comparisons, echo=TRUE}
test_data_model_emmeans <- emmeans(test_data_model_glm,
        specs = trt.vs.ctrl ~ rewarding.object.colour:trial,
        adjust = "mvt",
        by = "rewarding.object.colour") 
```

```{r model-3-contrasts-table, echo=FALSE}
test_data_contrasts <- as.data.frame(
  test_data_model_emmeans$contrasts %>% summary(infer = TRUE)
  ) %>% 
  mutate_if(is.numeric, round, digits = 3) 

test_data_contrasts$p.value <- format_p_value(test_data_contrasts$p.value)

test_data_contrasts$contrast <- c("Probe 1 - Initial",
  "Generalization 1 - Initial",
  "Generalization 2 - Initial",
  "Probe 2 - Initial",
  "Odour - Initial",
  "Probe 1 - Initial",
  "Generalization 1 - Initial",
  "Generalization 2 - Initial",
  "Probe 2 - Initial",
  "Odour - Initial")

test_data_contrasts %>%
  rename(
    "Contrast" = contrast,
    "Rewarding object colour" = rewarding.object.colour,
    "Estimate" = estimate,
    "Std. Error" = SE,
    "T ratio" = t.ratio,
    "P Value" = p.value,
    "Lower CL" = lower.CL,
    "Upper CL" = upper.CL
  ) %>%
  kable(
    caption = "Table of post-hoc tests with a multivariate t adjustment for multiple 
comparisons. Initial = Initial test (trial 0). Probe 1 = First probe test with object pair A (trial 21). Generalization 1 = Generalization to similar Lego colours test (trial 23). Generalization 2 = Generalization to non-Lego clay objects (trial 25). Probe 2 = Second probe test with object pair A (trial 27). Odour = Probe test with object pair A and food odour present on the food strips for both objects. The colour corresponds to the identity of the 
rewarding object (blue for blue-rewarded guppies, green for green-rewarded 
guppies). Values are all rounded to 3 decimal places."
  )
```

We also computed simple contrasts for the estimated marginal means of the
rewarding object preference between green-trained guppies and blue-trained
guppies within the same test trial to see if performance significantly differed
between the two groups during a test trial.

```{r}
test_data_simple_contrasts <- contrast(test_data_model_emmeans$emmeans, 
         method = "revpairwise", 
         simple = "rewarding.object.colour")
```

```{r, echo=FALSE}
test_data_simple_contrasts <- as.data.frame(
  test_data_simple_contrasts %>% summary(infer = TRUE)
  ) %>% 
    mutate_if(is.numeric, round, digits = 3) 

test_data_simple_contrasts$p.value <- 
  format_p_value(test_data_simple_contrasts$p.value)

test_data_simple_contrasts$contrast <- c(rep("Green - Blue", times = 6))

test_data_simple_contrasts$trial <- c("Initial",
                                      "Probe 1",
                                      "Generalization 1",
                                      "Generalization 2",
                                      "Probe 2",
                                      "Odour")

test_data_simple_contrasts %>%
  rename(
    "Contrast" = contrast,
    "Trial" = trial,
    "Estimate" = estimate,
    "Std. Error" = SE,
    "T ratio" = t.ratio,
    "P Value" = p.value,
    "Lower CL" = lower.CL,
    "Upper CL" = upper.CL
  ) %>%
  kable(
    caption = "Table of simple contrasts of rewarding object preference between colour treatments across each test trial. Initial = Initial test (trial 0). Probe 1 = First probe test with object pair A (trial 21). Generalization 1 = Generalization to similar Lego colours test (trial 23). Generalization 2 = Generalization to non-Lego clay objects (trial 25). Probe 2 = Second probe test with object pair A (trial 27). Odour = Probe test with object pair A and food odour present on the food strips for both objects. Values are all rounded to 3 decimal places. CL = confidence limit."
  )
```

Both rewarding object colour treatments showed evidence of having learned during
the first probe trial. Blue-trained guppies significantly increased their
preference for the rewarding object by
`r test_data_contrasts$estimate[1] %>% round(0)` seconds (p =
`r test_data_contrasts$p.value[1]`,
`r report_ci(test_data_contrasts, rounding = 2)[1]`) and green-trained guppies
significantly increased their preference for the rewarding object by
`r test_data_contrasts$estimate[6] %>% round(0)` seconds (p =
`r test_data_contrasts$p.value[6]`,
`r report_ci(test_data_contrasts, rounding = 2)[6]`). There is no difference in
performance between the two colour treatments in this test (p =
`r test_data_simple_contrasts$p.value[2]`,
`r report_ci(test_data_simple_contrasts, rounding = 2)[2]`).

Neither group displayed evidence of generalizing the learned colour preference
to a Lego object with a darker blue or green colouration (blue: p =
`r test_data_contrasts$p.value[2]`,
`r report_ci(test_data_contrasts, rounding = 2)[2]`, green: p =
`r test_data_contrasts$p.value[7]`,
`r report_ci(test_data_contrasts, rounding = 2)[7]`) and the groups did not
perform differently on this test (p = `r test_data_simple_contrasts$p.value[3]`,
`r report_ci(test_data_simple_contrasts, rounding = 2)[3]`).

Green-trained guppies showed evidence of generalizing their learned preference
to a non-Lego clay object (p = `r test_data_contrasts$p.value[8]`,
`r report_ci(test_data_contrasts, rounding = 2)[8]`), displaying a change in
preference of `r test_data_contrasts$estimate[8] %>% round(0)` seconds whereas
blue-trained guppies did not show evidence of a learned preference (p =
`r test_data_contrasts$p.value[3]`,
`r report_ci(test_data_contrasts, rounding = 2)[3]`). There is a significant
difference in performance between the two colour treatments in this test with
green-trained guppies displaying a superior performance over blue-trained
guppies (p = `r test_data_simple_contrasts$p.value[4]`,
`r report_ci(test_data_simple_contrasts, rounding = 2)[4]`).

For the second probe test to see if the learned preference would habituate with
a repeated test, green-trained guppies displayed limited evidence of having
retained their learned preference maintaining a shift in preference of
`r test_data_contrasts$estimate[9] %>% round(0)` seconds (p =
`r test_data_contrasts$p.value[9]`,
`r report_ci(test_data_contrasts, rounding = 2)[9]`) but blue-trained guppies
did not (p = `r test_data_contrasts$p.value[4]`,
`r report_ci(test_data_contrasts, rounding = 2)[4]`). There is a trend for
green-trained guppies to outperform blue-trained guppies in this test but this
is not significant ( p = `r test_data_simple_contrasts$p.value[5]`,
`r report_ci(test_data_simple_contrasts, rounding = 2)[5]`).

For the odour test green-trained guppies showed evidence of learning, displaying
a shift in preference of `r test_data_contrasts$estimate[10] %>% round(0)`
seconds (p `r test_data_contrasts$p.value[10]`,
`r report_ci(test_data_contrasts, rounding = 2)[10]`) but blue-trained guppies
did not (p = `r test_data_contrasts$p.value[5]`,
`r report_ci(test_data_contrasts, rounding = 2)[5]`) displaying a
non-significant shift of `r test_data_contrasts$estimate[5] %>% round(0)`
seconds. There is a significant difference in performance between the two colour
treatments in this test with green-trained guppies displaying a superior
performance over blue (p `r test_data_simple_contrasts$p.value[6]`,
`r report_ci(test_data_simple_contrasts, rounding = 2)[6]`).

Thus, in three out of five test trials green-trained guppies showed evidence of
learning while blue-trained guppies show evidence of learning in only one out of
five test trials with this trial being the very first probe trial after
training. Moreover, in two out of five test trials green-trained guppies
outperform blue-trained guppies and in a third test green-trained guppies trend
towards outperforming blue-trained guppies. Meanwhile blue-trained guppies never
outperform green-trained guppies.

```{r test-data-pref-plot, echo=FALSE, message=FALSE, fig.cap="Rewarding object preferences for an initial test prior to training and 5 probe tests after training. During training, fish were rewarded for approaching the blue object (blue squares) or the green object (green circles). At test, no food reward was present. Dashed line represents an equal preference for either object. Large blue squares and large green circles are means ± 95% CI, smaller blue squares and green circles are individual data points", fig.id="test-data-pref-plot", warning=FALSE, message=FALSE}
testing_data_x_axis_labels <- c("Initial", "Probe 1", "G1", "G2", "Probe 2", "Odour")

ggplot(
  test_data, 
  aes(
    x = trial, 
    y = rewarding.object.preference, 
    colour = rewarding.object.colour,
    shape = rewarding.object.colour
  )
) +
  theme_minimal() +
  geom_jitter(alpha = 0.5, size = 2,
              position = position_dodge(width = 0.5)) +
  stat_summary(geom = "point", fun = "mean", size = 4, 
               position = position_dodge(width = 0.5)) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci",
    position = position_dodge(width = 0.5),
    width = 0.3
  ) +
  ylab("Rewarding object preference (sec)") +
  xlab("Test") +
  labs(col = "Rewarding object colour") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5),
    strip.text.x = element_text(size = 14, face = "bold")
  ) +
  scale_x_discrete(labels = testing_data_x_axis_labels) +
  scale_color_manual(values = c("#2980b9", "#27ae60")) +
  scale_shape_manual(values = c(15, 16)) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.6) +
  scale_y_continuous(breaks = seq(-300, 300, by = 25))

ggsave(
  filename = "exp-2-model-3-test-data-pref-plot.png",
  path = "figs/exp-2/",
  device = "png",
  dpi = 300
)
```

***

### Model 4 - Is there a difference in feeding attempts between treatments? {#model-4}

A discrepancy in reinforcement between treatments may influence performance on a
final preference test. To see whether there was a difference in feeding between
treatments we counted the number of trials in which an individual fish ate
throughout all of training and compared the feeding counts between treatments.
To do this we fit a generalized linear model with a negative binomial
distribution. The response variable 'feeding count' is a sum of the number of
trials in which a guppy ate.

```{r model-4, echo=TRUE}
feeding_data_model <-
  glm.nb(training.feeding.count ~ rewarding.object.colour,
    data = feeding_data %>% filter(training.feeding.count > 0)
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName4"> See Model 4 Residuals </button>  
<div id="BlockName4" class="collapse">  

```{r, warning=FALSE, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = feeding_data_model)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "exp-2-model-4-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/exp-2/exp-2-residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>

```{r tidying-model-4, echo=FALSE, message=FALSE}
# Setting table row names
feeding_model_table_row_name_vec <- c(
  "Intercept",
  "Rewarding object colour"
)

# Converting data frame to tibble
tidy_feeding_data_model <- broom.mixed::tidy(feeding_data_model)

# Getting model confidence intervals
feeding_data_model_confint <- tibble::as_tibble((feeding_data_model %>%
  confint()), rownames = "factor")

# Changing tibble header names
tidy_feeding_data_model <- rename_tidy_lme4_cols(tidy_feeding_data_model)

# Changing tibble row names
tidy_feeding_data_model[1:2, 1] <- feeding_model_table_row_name_vec
```

\

##### Results

```{r, results=TRUE, echo=FALSE}
knitr::kable(tidy_feeding_data_model[1:2, ] %>%
  mutate_if(is.numeric, round, digits = 3))
```

We found no significant difference in the number of trials individuals fed
between green-rewarded and blue-rewarded fish (Figure
\@ref(fig:feeding-count-plot), p =
`r tidy_feeding_data_model$'P value'[2] %>% round(3)`).

```{r feeding-count-plot, echo=FALSE, message=FALSE, fig.cap="Average number of trials in which a fish fed during training. Data are means ± 95% confidence intervals with probability density functions of the data to the right of the raw data.", fig.id="training-data-ate-plot", warning=FALSE}

ggplot(
  feeding_data %>% filter(training.feeding.count > 0),
  aes(
    x = rewarding.object.colour,
    y = training.feeding.count,
    fill = rewarding.object.colour,
    colour = rewarding.object.colour
  )
) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.8) +
  geom_flat_violin(
    aes(fill = rewarding.object.colour),
    position = position_nudge(x = .25, y = 0),
    adjust = 0.7,
    alpha = 0.4,
    trim = FALSE,
    color = NA
  ) +
  stat_summary(geom = "point", fun = "mean", size = 4.5, shape = 15) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci",
    position = position_dodge(width = 0),
    width = 0.1
  ) +
  ylim(-3, 23) +
  ylab("Rewarding object preference") +
  xlab("Trial") +
  theme_minimal() +
  guides(fill = "none", colour = "none") +
  ylab("Number of trials fed") +
  xlab("Rewarding object colour") +
  labs(col = "Rewarding object colour") +
  theme(
    legend.position = "top",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5)
  ) +
  scale_color_manual(values = c("#2980b9", "#27ae60")) +
  scale_fill_manual(values = c("#2980b9", "#27ae60"))

# Saving plot to figs directory
ggsave(
  filename = "exp-2-model-4-feeding-data-plot.png",
  path = "figs/exp-2/",
  device = "png",
  dpi = 300
)
```

***

## ESM Models

### ESM Model 1 - Percentage preference for the rewarded object during testing depending on treatment

A reviewer of our manuscript asks 

>Why the authors did not (also) exploit a percentage preference, which is commonly used?

To determine whether our results are robust despite our different measure we
also ran an analysis with the percentage preference as an ESM Model. The main
pattern of results from the main experiment remain robust when we do this with
slight differences.

Since our data are continuous proportions we used a generalized linear mixed
effect model with a beta distribution to model these data. Beta distributions do
not accept 0s and 1s and, in our 90 data points, we had a single occurrence
(Trial 23, Generalization 1, individual 2a) of an individual spending 100% of
their time spent near an object near one object. A common transformation is to
convert 0s to 0.001 and 1s to 0.999 to control for this and thus we converted
this value of 1 to 0.999 to meet distributional assumptions.

```{r}
prop_test_data_model_glm <-  
  glmmTMB(prop.rewarding.object.preference ~  
            trial * rewarding.object.colour + (1|id), 
  data = test_data %>%
    mutate(
      prop.rewarding.object.preference =
        case_when(
          prop.rewarding.object.preference == 1 ~ 0.999,
          prop.rewarding.object.preference != 1 ~ prop.rewarding.object.preference
        )
    ), 
  family = beta_family(link="logit")
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName7"> See ESM Model 1 Residuals </button>  
<div id="BlockName7" class="collapse">  

```{r, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = prop_test_data_model_glm)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "exp-2-ESM-model-1-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/exp-2/exp-2-residual-plots/",
  device = "png",
  dpi = 300
)
```

</div>

\

##### Results

We once again produce a type-III ANOVA table based on Wald chi-square tests,
this time for ESM Model 1.

```{r, eval=FALSE}
car::Anova(prop_test_data_model_glm, type = "III")
```

```{r, echo=FALSE, message=FALSE}
esm_model_1_anova_table <- car::Anova(prop_test_data_model_glm, type = "III") 


rownames(esm_model_1_anova_table) <- c("Intercept", "Trial", 
                                   "Rewarding object colour", 
                                   "Trial X Rewarding object colour")
esm_model_1_anova_table %>% 
  tibble::rownames_to_column(var = "Factor") %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  rename("P Value" = 4) %>% 
  kable(caption = "ANOVA table for the test trials with proportional data")
```

As in [Model 3](#model-3) we find there there is an interaction effect between
trial and rewarding object colour for the test trials (p =
`r esm_model_1_anova_table$'Pr(>Chisq)'[4] %>% round(3)`). We again conduct
post-hoc tests as described in the [Model 3](#model-3) section.

```{r}
prop_test_data_model_emmeans <- emmeans(prop_test_data_model_glm,
        specs = trt.vs.ctrl ~ rewarding.object.colour:trial,
        adjust = "mvt",
        by = "rewarding.object.colour")
```


```{r prop-test-data-contrasts-table, echo=FALSE}
prop_test_data_contrasts <- as.data.frame(prop_test_data_model_emmeans$contrasts) %>% 
  mutate_if(is.numeric, round, digits = 3) 

prop_test_data_contrasts$p.value <- format_p_value(prop_test_data_contrasts$p.value)

prop_test_data_contrasts$contrast <- c("Probe 1 - Initial",
  "Generalization 1 - Initial",
  "Generalization 2 - Initial",
  "Probe 2 - Initial",
  "Odour - Initial",
  "Probe 1 - Initial",
  "Generalization 1 - Initial",
  "Generalization 2 - Initial",
  "Probe 2 - Initial",
  "Odour - Initial")

prop_test_data_contrasts %>%
  rename(
    "Contrast" = contrast,
    "Rewarding object colour" = rewarding.object.colour,
    "Estimate" = estimate,
    "Std. Error" = SE,
    "T ratio" = t.ratio,
    "P Value" = p.value
  ) %>%
  kable(
    caption = "Table of post-hoc tests with a multivariate t adjustment for multiple 
comparisons. Initial = Initial test (trial 0). Probe 1 = First probe test with object pair A (trial 21). Generalization 1 = Generalization to similar Lego colours test (trial 23). Generalization 2 = Generalization to non-Lego clay objects (trial 25). Probe 2 = Second probe test with object pair A (trial 27). Odour = Probe test with object pair A and food odour present on the food strips for both objects. The colour corresponds to the identity of the 
rewarding object (blue for blue-rewarded guppies, green for green-rewarded 
guppies). Values are all rounded to 3 decimal places."
  )
```

For full results see Table \@ref(tab:prop-test-data-contrasts-table). With the
proportional data we again find an overall interaction effect between trial and
rewarding object colour (p =
`r esm_model_1_anova_table$'Pr(>Chisq)'[4] %>% round(3)`) find that there is no
strong evidence for learning on the first probe test trial for both colour
treatments (blue: p = `r prop_test_data_contrasts$p.value[1]`, green: p =
`r prop_test_data_contrasts$p.value[6]`). This differs from the original model
in which both groups displayed evidence of having learned the association.

We find in the proportional data model that blue-trained guppies displayed
evidence of generalizing their colour preference to a Lego object of a similar
colour (p = `r prop_test_data_contrasts$p.value[2]`) but green-trained guppies
do not (p = `r prop_test_data_contrasts$p.value[7]`). In our original model
neither group displayed evidence of generalizing the learned colour preference
to a Lego object with a different blue and green colouration.

The pattern of results is the same between both models for the second
generalization trial where guppies were tested to see whether they could
generalize the colour prefernce to a non-Lego clay object. Green-trained guppies
showed evidence of generalizing their learned preference to a non-Lego clay
object (p = `r prop_test_data_contrasts$p.value[8]`), displaying a change in
preference of 21% whereas blue-trained guppies did not show evidence of a
learned preference (p = `r prop_test_data_contrasts$p.value[3]`).

For the second probe test to see if the learned preference would habituate with
a repeated test the pattern of results between both models is the same.
Green-trained guppies display limited evidence of having retained their learned
preference maintaining a non-significant shift in preference of 18% (p =
`r prop_test_data_contrasts$p.value[9]`) but blue-trained guppies do not (p =
`r prop_test_data_contrasts$p.value[4]`).

The odour test displays the same pattern of results as in the original model
green-trained guppies showed evidence of learning, displaying a shift in
preference of 25% (p = `r prop_test_data_contrasts$p.value[10]`) but
blue-trained guppies did not (p = `r prop_test_data_contrasts$p.value[5]`).

Thus, in this proportional data model green-trained guppies showed evidence of
learning in two out of five test trials while blue-trained guppies again show
evidence of learning in one out of five test trials, albeit a different test
trial. However, the trend for green to consistently outperform blue is
maintained in the effect sizes (Figure \@ref(fig:esm-model-1-plot)).

```{r esm-model-1-plot, echo=FALSE, message=FALSE, fig.cap="Rewarding object preferences for an initial test prior to training and 5 probe tests after training. During training, fish were rewarded for approaching the blue object (blue squares) or the green object (green circles). At test, no food reward was present. Dashed line represents an equal preference for either object. Large blue sqaures and large green circles are means ± 95% CI, smaller blue sqaures and green circles are individual data points"}
testing_data_x_axis_labels <- c("Initial", "Probe 1", "G1", "G2", "Probe 2", "Odour")

ggplot(
  test_data, 
  aes(
    x = trial, 
    y = prop.rewarding.object.preference, 
    colour = rewarding.object.colour,
    shape = rewarding.object.colour
  )
) +
  theme_minimal() +
  geom_jitter(alpha = 0.5, size = 2,
              position = position_dodge(width = 0.5)) +
  stat_summary(geom = "point", fun = "mean", size = 4, 
               position = position_dodge(width = 0.5)) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci",
    position = position_dodge(width = 0.5),
    width = 0.3
  ) +
  ylab("Rewarding object preference") +
  xlab("Test") +
  labs(col = "Rewarding object colour") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 14),
    axis.title = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5),
    strip.text.x = element_text(size = 14, face = "bold")
  ) +
  scale_x_discrete(labels = testing_data_x_axis_labels) +
  scale_color_manual(values = c("#2980b9", "#27ae60")) +
  scale_shape_manual(values = c(15, 16)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", alpha = 0.6) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.10))

ggsave(
  filename = "exp-2-ESM-model-1-percent-test-data-pref-plot.png",
  path = "figs/exp-2/",
  device = "png",
  dpi = 300
)
```


### ESM Model 2 - Does the object pair influence behaviour?

To determine whether guppies were treating object pairs differently from each
other during training (*e.g.*, avoiding larger object pairs), we asked whether
there was a difference in rewarding object preference among the object pairs
across all of the 20 training trials. To do this we fit a linear mixed effects
model with a response variable of rewarding object preference and a fixed
effect of the factor object pair. We then produced a type 3 ANOVA table of the
model.

```{r}
object_pair_model <-
  lmer(rewarding.object.preference ~ object.pair + (1 | id),
    data = training_data
  )
```

<button class="btn btn-primary" data-toggle="collapse" data-target="#BlockName8"> See ESM Model 2 Residuals </button>  
<div id="BlockName8" class="collapse">  

```{r, message=FALSE}
simulationOutput <- simulateResiduals(fittedModel = object_pair_model)
plot(simulationOutput)

# Saving plot to figs directory
ggsave(
  filename = "exp-2-ESM-model-2-residual-plot.png",
  plot = (plot(simulationOutput)),
  path = "figs/exp-2/exp-2-residual-plots/",
  device = "png",
  dpi = 300
)
```

There is a significant deviation from uniformity as indicated by the significant
Kolmogorov-Smirnov test. However, this model has a particularly large sample
size (n = 300) so even slight deviations will be significant. Looking at the
effect size of the deviation (D = `r testUniformity(simulationOutput)[1]`) shows
that it is minor (D < 0.1) and visual inspection does not suggest large
deviations in the residuals so our model is still appropriate.

</div>

\

##### Results

```{r, eval=FALSE}
car::Anova(object_pair_model, type = "III")
```

```{r, message=FALSE, echo=FALSE}
object_pair_model_anova_table <- car::Anova(object_pair_model, type = "III") 

rownames(object_pair_model_anova_table) <- c("Intercept", "Object pair")

object_pair_model_anova_table %>% 
  tibble::rownames_to_column(var = "Factor") %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  rename("P Value" = 4) %>% 
  kable(caption = "Type 3 ANOVA table for the effect of object pair on rewarding object preference during training in experiment 2.")
```

We find no evidence that guppies spent a different amount of time near a
rewarding object based on which object pair it came from (p =
`r object_pair_model_anova_table$'Pr(>Chisq)'[2] %>% round(3)`). Plotting the
data reveals a slight trend for guppies to spend more time near object pair 4
(Figure \@ref(fig:object-pair-plot)A). However, we note that object
pair 4 is the only object pair to have 4 out of 5 of its presentations after the
halfway mark of training (trial 10, Figure
\@ref(fig:object-pair-plot)B). This means guppies were much more
experienced with the colour association task by the time object pair 4 began
regularly appearing and likely explains the slightly higher rewarding object
preference for trials where object pair 4 is present.

```{r object-pair-plot, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="(A) Relative preference for the rewarding object for all guppies across all training trials (trials 1-20). Large circles are means ± 95% CI, smaller circles are raw data points. To the right of the raw data are probability density functions displaying the distribution of the data. (B) Relative preference for the rewarding object for all guppies during each individual training trial (trials 1-20). Negative values represent more time spent with the unrewarding object, positive values indicate more time spent with the rewarding object. Light grey lines connect individuals across trials. Large coloured circles are trial means ± 95% CI.", fig.height=8.5, fig.width=7.5}
p1 <- ggplot(
  training_data,
  aes(
    x = object.pair,
    y = rewarding.object.preference,
    color = object.pair
  )
) +
  theme_minimal() +
  ylab("Rewarding object preference (s)") +
  xlab("Object pair") +
  labs(col = "Object pair") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5),
    panel.border = element_rect(colour = "#bab0ac", fill = NA, size = 0.5),
    axis.line.x = element_blank(),
    axis.line.y = element_blank(),
    strip.text.x = element_text(size = 10, face = "bold")
  ) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci",
    position = position_dodge(width = 0.5),
    width = 0.15
  ) +
  geom_jitter(alpha = 0.3, width = 0.05) +
  scale_y_continuous(breaks = seq(-300, 300, by = 100)) +
  stat_summary(fun = "mean", size = 0.8) +
  geom_flat_violin(
    aes(fill = object.pair),
    position = position_nudge(x = .25, y = 0),
    adjust = 0.7,
    alpha = 0.4,
    trim = FALSE,
    color = NA
  ) +
  guides(fill = FALSE)

p2 <- ggplot(
  training_data,
  aes(
    x = trial,
    y = rewarding.object.preference,
    color = object.pair
  )
) +
  theme_minimal() +
  ylab("Rewarding object preference (s)") +
  xlab("Trial") +
  labs(col = "Object pair") +
  theme(
    legend.position = "none",
    axis.text = element_text(size = 12),
    axis.title = element_text(size = 13, face = "bold"),
    plot.title = element_text(size = 16, hjust = 0.5),
    panel.border = element_rect(colour = "#bab0ac", fill = NA, size = 0.5),
    axis.line.x = element_blank(),
    axis.line.y = element_blank()
  ) +
  stat_summary(
    geom = "errorbar",
    fun.data = "mean_ci",
    position = position_dodge(width = 0.5),
    width = 0.15
  ) +
  geom_jitter(alpha = 0.3, width = 0.05) +
  geom_line(aes(group = id), alpha = 0.2, color = "grey") +
  scale_x_continuous(breaks = c(1:20)) +
  scale_y_continuous(breaks = seq(-300, 300, by = 100)) +
  stat_summary(fun = "mean", size = 0.8)

# Combining both plots and adding figure labels 
ggarrange(p1, p2,
  heights =  c(1.3, 1),
  labels = "AUTO",
  label.x = c(0.1, 0.1),
  label.y = c(1, 1),
  hjust = -1,
  vjust = 2.5,
  nrow = 2,
  common.legend = TRUE
)

ggsave(
  filename = "exp-2-ESM-model-2-object-pair-plot.png",
  path = "figs/exp-2/",
  device = "png",
  dpi = 300
)
```

### ESM Model 3 - Is there a weight difference between the treatments?


We only need one trial in order to have the data needed to compare the weights
of the groups so we arbitrarily pick trial 0.

```{r}
weight_data <- full_data %>% filter(trial == 0)
```

The weights ranged from `r min(weight_data$weight)` to
`r max(weight_data$weight)` grams. Guppies weighed on average
`r weight_data %>% summarise(mean(weight)) %>% round(2)` grams. Blue-trained
guppies tended to weigh more than green-trained guppies
(`r weight_data %>% filter(rewarding.object.colour == "blue") %>% summarise(mean(weight)) %>% round(2)`
grams vs
`r weight_data %>% filter(rewarding.object.colour == "green") %>% summarise(mean(weight)) %>% round(2)`
grams) but this difference was not statistically significant when investigated
with a linear model (Table \@ref(tab:weight-model)).

```{r weight-model, echo=FALSE}
lm(weight ~ rewarding.object.colour, data = weight_data) %>%
  broom::tidy() %>%
  rename_tidy_lme4_cols() %>%
  mutate(
    Factor =
      case_when(
        Factor == "(Intercept)" ~ "Intercept",
        Factor == "rewarding.object.colourgreen" ~ "Rewarding object colour"
      )
  ) %>%
  mutate_if(is.numeric, round, digits = 3) %>%
  kable(caption = "Summary table for a linear model with a response variable of weight and a fixed effect of rewarding object colour.")
```

***

## Packages used 

The analyses on this page were done with `r R.version.string` and with functions
from packages listed in Table \@ref(tab:r-packages). This page was written in
Rmarkdown and rendered with `knitr`. In Table \@ref(tab:r-packages) we provide a
list of packages used, their versions, and references. Note this list does not
include the dependencies of these packages, it includes only packages
explicitly loaded with a `library()` call. Installing these packages would
automatically install all their dependencies but to see the full list of
dependencies for all packages used as well as their versions please visit the
[How to Reproduce the
Results](https://github.com/wyatt-toure/guppy-colour-learning#how-to-reproduce-the-results)
section of the README.

```{r, r-packages, echo=FALSE}
report(sessionInfo()) %>% 
  as.data.frame() %>% 
  kable(caption = "All packages used for this analysis. The dependencies for these packages as well as their versions can be found in the README file.")
```

***

## References


